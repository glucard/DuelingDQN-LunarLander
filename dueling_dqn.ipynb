{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45e981c7-3b8d-4dc1-b582-fed3d9c2098f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47aaad6d-c42e-4b41-a48c-dfbee713c5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\",render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aaaa656-9d92-47fb-93dd-15ff04ae3cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque\n",
    "import random\n",
    "\n",
    "Transition = namedtuple('Transition', ('state', 'action','next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory():\n",
    "\n",
    "  def __init__(self, capacity):\n",
    "    self.memory = deque([], maxlen=capacity)\n",
    "  \n",
    "  def push(self, *args):\n",
    "    self.memory.append(Transition(*args))\n",
    "  \n",
    "  def sample(self, batch_size):\n",
    "    return random.sample(self.memory, batch_size)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35dd014e-a2da-45c7-b064-e2e6ef548f2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DQN(nn.Module):\n",
    "  def __init__(self, n_observations, n_actions):\n",
    "    super(DQN, self).__init__()\n",
    "    self.layer_input = nn.Linear(n_observations, 512)\n",
    "    self.layer_h_1 = nn.Linear(512, 512)\n",
    "    self.layer_h_2 = nn.Linear(512, 512)\n",
    "    self.layer_v = nn.Linear(512, 1)\n",
    "    self.layer_a = nn.Linear(512, n_actions)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.layer_input(x))\n",
    "    # x = self.dropout0(x)\n",
    "    x = F.relu(self.layer_h_1(x))\n",
    "    # x = self.dropout1(x)\n",
    "    x = F.relu(self.layer_h_2(x))\n",
    "    # x = self.dropout2(x)\n",
    "    \n",
    "    v = self.layer_v(x)\n",
    "    a = self.layer_a(x)\n",
    "    \n",
    "    q = v + a - a.mean()\n",
    "    \n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1730ff05-7d09-4b1a-bc31-5980fd0f684d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "REPLAY_SIZE = 100_000\n",
    "BATCH_SIZE = 1024\n",
    "GAMMA = 0.99\n",
    "TAU = 0.005\n",
    "LR = 0.00005\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LR) # amsgrad? r:\n",
    "memory = ReplayMemory(REPLAY_SIZE)\n",
    "global_step = 0\n",
    "\n",
    "def select_action(state, eps_threshold):\n",
    "  #global steps_done\n",
    "  sample = random.random()\n",
    "\n",
    "  if sample > eps_threshold:\n",
    "    with torch.no_grad():\n",
    "      return policy_net(state).max(1)[1].view(1,1) # .view(1,1)? r:\n",
    "  else:\n",
    "    return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions)) # print after\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device)\n",
    "\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch) # print after\n",
    "\n",
    "\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "    expected_next_action_values = reward_batch + GAMMA * next_state_values\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    loss = criterion(state_action_values, expected_next_action_values.unsqueeze(1))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a259973f-0475-4f15-8819-7e0666a22382",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 80 #6/1\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  num_episodes = 200\n",
    "else:\n",
    "  num_episodes = 50\n",
    "\n",
    "h_params = {\n",
    "    'EPS_START': EPS_START,\n",
    "    'EPS_END': EPS_END,\n",
    "    'EPS_DECAY': EPS_DECAY,\n",
    "    'REPLAY_SIZE': REPLAY_SIZE,\n",
    "    'BATCH_SIZE': BATCH_SIZE,\n",
    "    'GAMMA': GAMMA,\n",
    "    'TAU': TAU,\n",
    "    'LR': LR,\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "feefb3ab-f2d1-4121-bc97-1aa85fa1bac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "timestr = time.strftime(\"%Y_%m_%d_%H_%M_%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40ac0bd0-bb81-4cf0-8f94-4cd6419d8fbf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/200] [loss_mean: 126.54937744140625] [rewards_mean: -51.52707693836419] [iterations: 70] [last reward: -100.0]\n",
      "[1/200] [loss_mean: 99.88041684054589] [rewards_mean: -470.0474033802748] [iterations: 128] [last reward: -100.0]\n",
      "[2/200] [loss_mean: 60.81026342405495] [rewards_mean: -252.43054363131523] [iterations: 140] [last reward: -100.0]\n",
      "[3/200] [loss_mean: 42.26033146527349] [rewards_mean: -101.64349865505937] [iterations: 97] [last reward: -100.0]\n",
      "[4/200] [loss_mean: 33.196225560944654] [rewards_mean: -120.84520431328565] [iterations: 144] [last reward: -100.0]\n",
      "[5/200] [loss_mean: 32.71149365764019] [rewards_mean: -351.92865830659866] [iterations: 120] [last reward: -100.0]\n",
      "[6/200] [loss_mean: 33.23636688104197] [rewards_mean: -272.23162192478776] [iterations: 118] [last reward: -100.0]\n",
      "[7/200] [loss_mean: 32.598240768516455] [rewards_mean: -164.82983389496803] [iterations: 90] [last reward: -100.0]\n",
      "[8/200] [loss_mean: 29.66788767646341] [rewards_mean: -119.6488358611241] [iterations: 84] [last reward: -100.0]\n",
      "[9/200] [loss_mean: 27.152877910078065] [rewards_mean: -192.74751539342105] [iterations: 120] [last reward: -100.0]\n",
      "[10/200] [loss_mean: 22.11198641942895] [rewards_mean: -120.84864594787359] [iterations: 137] [last reward: -100.0]\n",
      "[11/200] [loss_mean: 20.359211932781133] [rewards_mean: -356.11359903309494] [iterations: 128] [last reward: -100.0]\n",
      "[12/200] [loss_mean: 20.61131309534048] [rewards_mean: -142.52628692239523] [iterations: 76] [last reward: -100.0]\n",
      "[13/200] [loss_mean: 18.92011631567647] [rewards_mean: -262.7078632246703] [iterations: 126] [last reward: -100.0]\n",
      "[14/200] [loss_mean: 18.716014500322014] [rewards_mean: -81.07827860640828] [iterations: 115] [last reward: -100.0]\n",
      "[15/200] [loss_mean: 15.536894168573268] [rewards_mean: -49.277392750023864] [iterations: 169] [last reward: -100.0]\n",
      "[16/200] [loss_mean: 12.45942457109435] [rewards_mean: -104.895208530128] [iterations: 116] [last reward: -100.0]\n",
      "[17/200] [loss_mean: 14.00468356008748] [rewards_mean: -33.30628228653222] [iterations: 130] [last reward: -100.0]\n",
      "[18/200] [loss_mean: 15.280794201578413] [rewards_mean: -276.348814798519] [iterations: 139] [last reward: -100.0]\n",
      "[19/200] [loss_mean: 19.307763737898608] [rewards_mean: -130.29232925642282] [iterations: 129] [last reward: -100.0]\n",
      "[20/200] [loss_mean: 18.138418361875747] [rewards_mean: -66.9672241806984] [iterations: 89] [last reward: -100.0]\n",
      "[21/200] [loss_mean: 15.940476802679209] [rewards_mean: -21.784446202218533] [iterations: 77] [last reward: -100.0]\n",
      "[22/200] [loss_mean: 17.261841362358158] [rewards_mean: -127.47395551856607] [iterations: 116] [last reward: -100.0]\n",
      "[23/200] [loss_mean: 17.90447417125907] [rewards_mean: -94.19988283404382] [iterations: 92] [last reward: -100.0]\n",
      "[24/200] [loss_mean: 17.460904219571283] [rewards_mean: -48.14985559694469] [iterations: 67] [last reward: -100.0]\n",
      "[25/200] [loss_mean: 17.27232615152995] [rewards_mean: -122.76861034333706] [iterations: 116] [last reward: -100.0]\n",
      "[26/200] [loss_mean: 14.757663351857763] [rewards_mean: -74.38631029240787] [iterations: 116] [last reward: -100.0]\n",
      "[27/200] [loss_mean: 14.136191202093054] [rewards_mean: -16.301372565794736] [iterations: 134] [last reward: -100.0]\n",
      "[28/200] [loss_mean: 14.229285260682465] [rewards_mean: -58.53464411944151] [iterations: 92] [last reward: -100.0]\n",
      "[29/200] [loss_mean: 13.731317437056338] [rewards_mean: -207.25778152048588] [iterations: 131] [last reward: -100.0]\n",
      "[30/200] [loss_mean: 13.745097273647195] [rewards_mean: -66.97734507592395] [iterations: 100] [last reward: -100.0]\n",
      "[31/200] [loss_mean: 13.505152203486515] [rewards_mean: -60.330282447859645] [iterations: 129] [last reward: -100.0]\n",
      "[32/200] [loss_mean: 12.426626677513122] [rewards_mean: -91.99980503506958] [iterations: 199] [last reward: -100.0]\n",
      "[33/200] [loss_mean: 12.960413452295157] [rewards_mean: 10.130173729732633] [iterations: 129] [last reward: -100.0]\n",
      "[34/200] [loss_mean: 12.188470571469038] [rewards_mean: -104.21178128896281] [iterations: 350] [last reward: -100.0]\n",
      "[35/200] [loss_mean: 11.772445636512959] [rewards_mean: -54.33730714023113] [iterations: 112] [last reward: -100.0]\n",
      "[36/200] [loss_mean: 12.379192047259387] [rewards_mean: -55.1597591266036] [iterations: 135] [last reward: -100.0]\n",
      "[37/200] [loss_mean: 12.425839176884404] [rewards_mean: -40.12185809388757] [iterations: 80] [last reward: -100.0]\n",
      "[38/200] [loss_mean: 10.879544149745595] [rewards_mean: -87.1734898481518] [iterations: 164] [last reward: -100.0]\n",
      "[39/200] [loss_mean: 10.144969199014747] [rewards_mean: -31.683897057082504] [iterations: 91] [last reward: -100.0]\n",
      "[40/200] [loss_mean: 9.58746327672686] [rewards_mean: -93.70372312364634] [iterations: 76] [last reward: -100.0]\n",
      "[41/200] [loss_mean: 9.575135105946025] [rewards_mean: -64.20603070780635] [iterations: 121] [last reward: -100.0]\n",
      "[42/200] [loss_mean: 9.405223519802094] [rewards_mean: -84.53554947683006] [iterations: 199] [last reward: -100.0]\n",
      "[43/200] [loss_mean: 9.048782706260681] [rewards_mean: 12.003368221223354] [iterations: 157] [last reward: -100.0]\n",
      "[44/200] [loss_mean: 8.524106683209538] [rewards_mean: -52.22031312994659] [iterations: 127] [last reward: -100.0]\n",
      "[45/200] [loss_mean: 8.50204516906106] [rewards_mean: -61.02162865921855] [iterations: 180] [last reward: -100.0]\n",
      "[46/200] [loss_mean: 8.70149679934041] [rewards_mean: -4.911318428814411] [iterations: 88] [last reward: -100.0]\n",
      "[47/200] [loss_mean: 8.043996493750756] [rewards_mean: -44.083163722156314] [iterations: 217] [last reward: -100.0]\n",
      "[48/200] [loss_mean: 7.516593113651982] [rewards_mean: -0.7414227044209838] [iterations: 134] [last reward: -100.0]\n",
      "[49/200] [loss_mean: 7.8271910848780575] [rewards_mean: 15.116794407367706] [iterations: 233] [last reward: -100.0]\n",
      "[50/200] [loss_mean: 7.6753339447926] [rewards_mean: -34.83578725619009] [iterations: 96] [last reward: -100.0]\n",
      "[51/200] [loss_mean: 7.736820109122623] [rewards_mean: -149.2988798711449] [iterations: 112] [last reward: -100.0]\n",
      "[52/200] [loss_mean: 7.446840755317522] [rewards_mean: -33.65931777982041] [iterations: 91] [last reward: -100.0]\n",
      "[53/200] [loss_mean: 7.631532411106297] [rewards_mean: -21.7979757739231] [iterations: 121] [last reward: -100.0]\n",
      "[54/200] [loss_mean: 7.375616205705179] [rewards_mean: 9.717725288122892] [iterations: 147] [last reward: -100.0]\n",
      "[55/200] [loss_mean: 7.626933701494907] [rewards_mean: -52.02662695012987] [iterations: 140] [last reward: -100.0]\n",
      "[56/200] [loss_mean: 7.790155937671662] [rewards_mean: -64.72652861475945] [iterations: 99] [last reward: -100.0]\n",
      "[57/200] [loss_mean: 6.63450146236008] [rewards_mean: -26.255141757428646] [iterations: 138] [last reward: -100.0]\n",
      "[58/200] [loss_mean: 7.225640674806991] [rewards_mean: -42.253311158390716] [iterations: 105] [last reward: -100.0]\n",
      "[59/200] [loss_mean: 7.560533633938542] [rewards_mean: -26.744215600192547] [iterations: 107] [last reward: -100.0]\n",
      "[60/200] [loss_mean: 7.343896823316007] [rewards_mean: 9.737932592630386] [iterations: 184] [last reward: -100.0]\n",
      "[61/200] [loss_mean: 6.712120178526482] [rewards_mean: -90.75783034227788] [iterations: 225] [last reward: -100.0]\n",
      "[62/200] [loss_mean: 7.030023166503029] [rewards_mean: -32.22965922951698] [iterations: 86] [last reward: -100.0]\n",
      "[63/200] [loss_mean: 7.462823067254167] [rewards_mean: -11.04913118854165] [iterations: 122] [last reward: -100.0]\n",
      "[64/200] [loss_mean: 6.84117401273627] [rewards_mean: -101.51884041726589] [iterations: 151] [last reward: -100.0]\n",
      "[65/200] [loss_mean: 6.868447020337298] [rewards_mean: -118.92975122109056] [iterations: 142] [last reward: -100.0]\n",
      "[66/200] [loss_mean: 6.8614216327667235] [rewards_mean: -145.1104577165097] [iterations: 199] [last reward: -100.0]\n",
      "[67/200] [loss_mean: 7.436154480221905] [rewards_mean: -106.19124694215134] [iterations: 157] [last reward: -100.0]\n",
      "[68/200] [loss_mean: 7.001404879044514] [rewards_mean: -90.97813070751727] [iterations: 146] [last reward: -100.0]\n",
      "[69/200] [loss_mean: 7.155510017165431] [rewards_mean: -52.17819637618959] [iterations: 107] [last reward: -100.0]\n",
      "[70/200] [loss_mean: 8.042633021396139] [rewards_mean: -10.837887220084667] [iterations: 114] [last reward: -100.0]\n",
      "[71/200] [loss_mean: 7.351535191045743] [rewards_mean: -48.05208309739828] [iterations: 106] [last reward: -100.0]\n",
      "[72/200] [loss_mean: 7.132122533886056] [rewards_mean: -64.72905503213406] [iterations: 151] [last reward: -100.0]\n",
      "[73/200] [loss_mean: 7.115916905885047] [rewards_mean: -94.25535647571087] [iterations: 375] [last reward: -100.0]\n",
      "[74/200] [loss_mean: 7.4864635594685875] [rewards_mean: 4.70844616740942] [iterations: 149] [last reward: -100.0]\n",
      "[75/200] [loss_mean: 7.003710235868182] [rewards_mean: -84.79120590351522] [iterations: 125] [last reward: -100.0]\n",
      "[76/200] [loss_mean: 7.455322994376129] [rewards_mean: -34.756014317274094] [iterations: 105] [last reward: -100.0]\n",
      "[77/200] [loss_mean: 7.475969478368759] [rewards_mean: 3.0555706961094984] [iterations: 999] [last reward: 2.2920968532562256]\n",
      "[78/200] [loss_mean: 7.361313693881653] [rewards_mean: -202.37279135920107] [iterations: 192] [last reward: -100.0]\n",
      "[79/200] [loss_mean: 7.047335357863668] [rewards_mean: -149.51856975606643] [iterations: 192] [last reward: -100.0]\n",
      "[80/200] [loss_mean: 8.217839087758746] [rewards_mean: -21.89881321787834] [iterations: 97] [last reward: -100.0]\n",
      "[81/200] [loss_mean: 7.945750982203382] [rewards_mean: -159.7795645808801] [iterations: 328] [last reward: -100.0]\n",
      "[82/200] [loss_mean: 8.260780740667272] [rewards_mean: -31.59996245475486] [iterations: 107] [last reward: -100.0]\n",
      "[83/200] [loss_mean: 8.189720520074816] [rewards_mean: -22.892504927702248] [iterations: 413] [last reward: -100.0]\n",
      "[84/200] [loss_mean: 7.995927829027176] [rewards_mean: -23.747377291787416] [iterations: 999] [last reward: 1.0606459379196167]\n",
      "[85/200] [loss_mean: 7.48884368521496] [rewards_mean: -28.776339459232986] [iterations: 205] [last reward: -100.0]\n",
      "[86/200] [loss_mean: 7.227108887378002] [rewards_mean: 6.033084336668253] [iterations: 297] [last reward: -100.0]\n",
      "[87/200] [loss_mean: 7.2244850730896] [rewards_mean: 71.02604189538397] [iterations: 999] [last reward: 2.120727777481079]\n",
      "[88/200] [loss_mean: 7.080990453561147] [rewards_mean: -36.9533102568239] [iterations: 107] [last reward: -100.0]\n",
      "[89/200] [loss_mean: 6.955038160324096] [rewards_mean: -9.783444192027673] [iterations: 999] [last reward: 2.538658857345581]\n",
      "[90/200] [loss_mean: 6.705363667200482] [rewards_mean: -54.66423371434212] [iterations: 125] [last reward: -100.0]\n",
      "[91/200] [loss_mean: 6.6332787445613315] [rewards_mean: 40.46726519614458] [iterations: 174] [last reward: -100.0]\n",
      "[92/200] [loss_mean: 6.987197080850601] [rewards_mean: -67.9056282939855] [iterations: 999] [last reward: -1.6015560626983643]\n",
      "[93/200] [loss_mean: 7.002220787579501] [rewards_mean: -49.23288984573446] [iterations: 157] [last reward: -100.0]\n",
      "[94/200] [loss_mean: 6.720500372409821] [rewards_mean: 6.388647836982273] [iterations: 999] [last reward: -2.6527745723724365]\n",
      "[95/200] [loss_mean: 6.713723103284836] [rewards_mean: 67.79390758264344] [iterations: 999] [last reward: 0.8900713324546814]\n",
      "[96/200] [loss_mean: 6.742420469045639] [rewards_mean: 53.981145061407005] [iterations: 999] [last reward: -0.954818069934845]\n",
      "[97/200] [loss_mean: 6.6269134533405305] [rewards_mean: -7.178146759979427] [iterations: 999] [last reward: -2.6360530853271484]\n",
      "[98/200] [loss_mean: 6.487756250858307] [rewards_mean: -37.16867492417805] [iterations: 999] [last reward: -3.6662344932556152]\n",
      "[99/200] [loss_mean: 6.432760568857193] [rewards_mean: 23.329755796980862] [iterations: 999] [last reward: 0.5668392777442932]\n",
      "[100/200] [loss_mean: 6.883396232843399] [rewards_mean: 47.429762321864445] [iterations: 999] [last reward: 0.42779603600502014]\n",
      "[101/200] [loss_mean: 6.758517162774199] [rewards_mean: -18.445120565593243] [iterations: 92] [last reward: -100.0]\n",
      "[102/200] [loss_mean: 6.454942718267441] [rewards_mean: 6.566850278672064] [iterations: 999] [last reward: 1.0395807027816772]\n",
      "[103/200] [loss_mean: 6.119067999601364] [rewards_mean: -90.6464336287172] [iterations: 999] [last reward: -0.32184359431266785]\n",
      "[104/200] [loss_mean: 6.219336181163788] [rewards_mean: 121.62963999985777] [iterations: 999] [last reward: 0.012914557941257954]\n",
      "[105/200] [loss_mean: 6.397601905335012] [rewards_mean: 143.48936462888378] [iterations: 961] [last reward: 100.0]\n",
      "[106/200] [loss_mean: 6.428311081886291] [rewards_mean: -67.39400854986161] [iterations: 999] [last reward: -1.0472491979599]\n",
      "[107/200] [loss_mean: 5.677136468887329] [rewards_mean: -63.57637582439929] [iterations: 999] [last reward: -2.318246603012085]\n",
      "[108/200] [loss_mean: 5.435465240955353] [rewards_mean: -55.2746082819649] [iterations: 999] [last reward: 0.08148419857025146]\n",
      "[109/200] [loss_mean: 5.338659122467041] [rewards_mean: 112.3216571437539] [iterations: 999] [last reward: 0.495381623506546]\n",
      "[110/200] [loss_mean: 5.518320718765259] [rewards_mean: 133.1451100619646] [iterations: 999] [last reward: 0.009742404334247112]\n",
      "[111/200] [loss_mean: 5.338261033773422] [rewards_mean: 72.48625395599811] [iterations: 999] [last reward: -0.06031890958547592]\n",
      "[112/200] [loss_mean: 5.032006416649654] [rewards_mean: -140.39872380439192] [iterations: 434] [last reward: -100.0]\n",
      "[113/200] [loss_mean: 5.1893789536952974] [rewards_mean: -68.99833688910803] [iterations: 999] [last reward: 0.563981294631958]\n",
      "[114/200] [loss_mean: 5.093779415607452] [rewards_mean: -45.727630438283086] [iterations: 999] [last reward: -2.5669002532958984]\n",
      "[115/200] [loss_mean: 5.104462106466293] [rewards_mean: 17.12742131736013] [iterations: 999] [last reward: -0.3913472294807434]\n",
      "[116/200] [loss_mean: 5.408433105227208] [rewards_mean: 1.60759849101305] [iterations: 228] [last reward: -100.0]\n",
      "[117/200] [loss_mean: 5.687492241382599] [rewards_mean: 95.51180232566549] [iterations: 999] [last reward: -17.57925796508789]\n",
      "[118/200] [loss_mean: 5.158519830448287] [rewards_mean: 202.4058052458687] [iterations: 559] [last reward: 100.0]\n",
      "[119/200] [loss_mean: 5.342061289189535] [rewards_mean: 238.0599969815729] [iterations: 835] [last reward: 100.0]\n",
      "[120/200] [loss_mean: 5.317494903564453] [rewards_mean: 120.81474667031381] [iterations: 999] [last reward: 0.07100033760070801]\n",
      "[121/200] [loss_mean: 5.392951315402985] [rewards_mean: 88.4576597121195] [iterations: 999] [last reward: 0.6105589270591736]\n",
      "[122/200] [loss_mean: 5.314963071346283] [rewards_mean: 107.28492612760112] [iterations: 999] [last reward: 9.469904899597168]\n",
      "[123/200] [loss_mean: 5.249296958629902] [rewards_mean: -27.468254461651668] [iterations: 220] [last reward: -100.0]\n",
      "[124/200] [loss_mean: 5.427597224263252] [rewards_mean: 194.22769536620763] [iterations: 928] [last reward: 100.0]\n",
      "[125/200] [loss_mean: 5.27361154961586] [rewards_mean: 112.61234612337648] [iterations: 999] [last reward: -2.201876401901245]\n",
      "[126/200] [loss_mean: 5.175960712888267] [rewards_mean: -209.50438933819532] [iterations: 355] [last reward: -100.0]\n",
      "[127/200] [loss_mean: 5.781947137117386] [rewards_mean: 49.37619385984726] [iterations: 999] [last reward: -2.479548454284668]\n",
      "[128/200] [loss_mean: 5.712158788401761] [rewards_mean: 265.20183838683914] [iterations: 320] [last reward: 100.0]\n",
      "[129/200] [loss_mean: 5.520970311409757] [rewards_mean: 268.5899806240048] [iterations: 992] [last reward: 100.0]\n",
      "[130/200] [loss_mean: 5.412967719554901] [rewards_mean: 122.59285180424502] [iterations: 999] [last reward: 0.155399352312088]\n",
      "[131/200] [loss_mean: 5.540118845701218] [rewards_mean: 103.82591362205858] [iterations: 999] [last reward: -0.25345852971076965]\n",
      "[132/200] [loss_mean: 5.335952794313431] [rewards_mean: 171.44386773130827] [iterations: 999] [last reward: 3.364917211001739e-05]\n",
      "[133/200] [loss_mean: 5.107920957648235] [rewards_mean: 268.9428302932693] [iterations: 229] [last reward: 100.0]\n",
      "[134/200] [loss_mean: 5.061935106209949] [rewards_mean: 287.86830616445593] [iterations: 451] [last reward: 100.0]\n",
      "[135/200] [loss_mean: 5.139812810897827] [rewards_mean: 166.88999891370725] [iterations: 999] [last reward: 12.127241134643555]\n",
      "[136/200] [loss_mean: 5.163328538164061] [rewards_mean: 271.5068312045958] [iterations: 341] [last reward: 100.0]\n",
      "[137/200] [loss_mean: 5.044200176954269] [rewards_mean: 91.39464447874002] [iterations: 999] [last reward: -1.9117459058761597]\n",
      "[138/200] [loss_mean: 4.636411398387025] [rewards_mean: 272.0932793273656] [iterations: 340] [last reward: 100.0]\n",
      "[139/200] [loss_mean: 4.944549698025788] [rewards_mean: 293.62982196174397] [iterations: 521] [last reward: 100.0]\n",
      "[140/200] [loss_mean: 5.377999676240457] [rewards_mean: 290.6729902168887] [iterations: 369] [last reward: 100.0]\n",
      "[141/200] [loss_mean: 5.2115469227234525] [rewards_mean: 171.12308794759883] [iterations: 767] [last reward: 100.0]\n",
      "[142/200] [loss_mean: 5.242324619019618] [rewards_mean: 267.86176819705366] [iterations: 243] [last reward: 100.0]\n",
      "[143/200] [loss_mean: 5.264805610583085] [rewards_mean: 235.87428011562884] [iterations: 750] [last reward: 100.0]\n",
      "[144/200] [loss_mean: 5.157298877698566] [rewards_mean: 284.4136309136611] [iterations: 435] [last reward: 100.0]\n",
      "[145/200] [loss_mean: 5.216025729179382] [rewards_mean: 255.08752322875702] [iterations: 249] [last reward: 100.0]\n",
      "[146/200] [loss_mean: 5.554071289664101] [rewards_mean: 279.21139596964156] [iterations: 278] [last reward: 100.0]\n",
      "[147/200] [loss_mean: 5.3718771064316835] [rewards_mean: 291.3910017833371] [iterations: 336] [last reward: 100.0]\n",
      "[148/200] [loss_mean: 5.694050140240613] [rewards_mean: 283.6820045494493] [iterations: 407] [last reward: 100.0]\n",
      "[149/200] [loss_mean: 5.659921090405686] [rewards_mean: 302.0654883873974] [iterations: 664] [last reward: 100.0]\n",
      "[150/200] [loss_mean: 5.786190339326859] [rewards_mean: 119.78703480222794] [iterations: 999] [last reward: -17.882362365722656]\n",
      "[151/200] [loss_mean: 5.8345652931141405] [rewards_mean: -26.100484204129316] [iterations: 158] [last reward: -100.0]\n",
      "[152/200] [loss_mean: 6.141110310077667] [rewards_mean: 106.37334189367175] [iterations: 999] [last reward: 0.051285140216350555]\n",
      "[153/200] [loss_mean: 5.854189422822768] [rewards_mean: 288.8214821666858] [iterations: 557] [last reward: 100.0]\n",
      "[154/200] [loss_mean: 5.738084663556316] [rewards_mean: 274.43910894743686] [iterations: 507] [last reward: 100.0]\n",
      "[155/200] [loss_mean: 6.103372880647767] [rewards_mean: 301.71147024412915] [iterations: 264] [last reward: 100.0]\n",
      "[156/200] [loss_mean: 6.136900228707969] [rewards_mean: 279.1691167062132] [iterations: 587] [last reward: 100.0]\n",
      "[157/200] [loss_mean: 6.14397131596353] [rewards_mean: 243.08334053713378] [iterations: 282] [last reward: 100.0]\n",
      "[158/200] [loss_mean: 5.865765855750259] [rewards_mean: 271.9518011188022] [iterations: 244] [last reward: 100.0]\n",
      "[159/200] [loss_mean: 6.198061279663549] [rewards_mean: 269.13031460139973] [iterations: 592] [last reward: 100.0]\n",
      "[160/200] [loss_mean: 6.154230183466493] [rewards_mean: 251.08461449268037] [iterations: 402] [last reward: 100.0]\n",
      "[161/200] [loss_mean: 5.8792527627341356] [rewards_mean: 291.7206123064312] [iterations: 236] [last reward: 100.0]\n",
      "[162/200] [loss_mean: 6.320677598533423] [rewards_mean: 289.42923018489904] [iterations: 367] [last reward: 100.0]\n",
      "[163/200] [loss_mean: 5.957449075954208] [rewards_mean: 241.50692580186578] [iterations: 365] [last reward: 100.0]\n",
      "[164/200] [loss_mean: 6.390367554293738] [rewards_mean: 295.4711698434122] [iterations: 287] [last reward: 100.0]\n",
      "[165/200] [loss_mean: 6.03956316399762] [rewards_mean: 250.3060932504073] [iterations: 380] [last reward: 100.0]\n",
      "[166/200] [loss_mean: 6.546381103391407] [rewards_mean: 239.31850628430414] [iterations: 475] [last reward: 100.0]\n",
      "[167/200] [loss_mean: 6.404151828955283] [rewards_mean: 262.4041365243084] [iterations: 432] [last reward: 100.0]\n",
      "[168/200] [loss_mean: 6.186439931506202] [rewards_mean: 297.6892092169902] [iterations: 524] [last reward: 100.0]\n",
      "[169/200] [loss_mean: 6.283862782312819] [rewards_mean: 200.66879041529234] [iterations: 604] [last reward: 100.0]\n",
      "[170/200] [loss_mean: 6.6084165619868855] [rewards_mean: 285.95362417961076] [iterations: 356] [last reward: 100.0]\n",
      "[171/200] [loss_mean: 6.536584275109427] [rewards_mean: 241.42657808290875] [iterations: 384] [last reward: 100.0]\n",
      "[172/200] [loss_mean: 6.260419781199731] [rewards_mean: 202.7572382767052] [iterations: 573] [last reward: 100.0]\n",
      "[173/200] [loss_mean: 6.406170428199256] [rewards_mean: 301.6644808661556] [iterations: 297] [last reward: 100.0]\n",
      "[174/200] [loss_mean: 6.569853798202846] [rewards_mean: 286.4761703465367] [iterations: 321] [last reward: 100.0]\n",
      "[175/200] [loss_mean: 6.3026922166347505] [rewards_mean: 128.59048482583785] [iterations: 999] [last reward: -0.06874144822359085]\n",
      "[176/200] [loss_mean: 6.428333955881547] [rewards_mean: 280.36417426978744] [iterations: 244] [last reward: 100.0]\n",
      "[177/200] [loss_mean: 6.685207882570842] [rewards_mean: 288.5675598765861] [iterations: 251] [last reward: 100.0]\n",
      "[178/200] [loss_mean: 6.524882157162711] [rewards_mean: 206.42200923664495] [iterations: 660] [last reward: 100.0]\n",
      "[179/200] [loss_mean: 6.625568789659544] [rewards_mean: 222.97757663825996] [iterations: 429] [last reward: 100.0]\n",
      "[180/200] [loss_mean: 6.734747765720755] [rewards_mean: 256.17431189680184] [iterations: 275] [last reward: 100.0]\n",
      "[181/200] [loss_mean: 7.215096311097431] [rewards_mean: 243.73543578889792] [iterations: 282] [last reward: 100.0]\n",
      "[182/200] [loss_mean: 6.960268613289703] [rewards_mean: 255.51463689366327] [iterations: 351] [last reward: 100.0]\n",
      "[183/200] [loss_mean: 7.404482525638026] [rewards_mean: 288.86154529517455] [iterations: 350] [last reward: 100.0]\n",
      "[184/200] [loss_mean: 6.892850480944954] [rewards_mean: 247.13064646688872] [iterations: 258] [last reward: 100.0]\n",
      "[185/200] [loss_mean: 6.94767222443565] [rewards_mean: 239.0248199852649] [iterations: 243] [last reward: 100.0]\n",
      "[186/200] [loss_mean: 6.953299748791432] [rewards_mean: 261.8548700179463] [iterations: 336] [last reward: 100.0]\n",
      "[187/200] [loss_mean: 7.398809143315966] [rewards_mean: 289.99059909767664] [iterations: 320] [last reward: 100.0]\n",
      "[188/200] [loss_mean: 7.283838733520798] [rewards_mean: 243.6398054698044] [iterations: 525] [last reward: 100.0]\n",
      "[189/200] [loss_mean: 7.409038823127746] [rewards_mean: 93.44498443775046] [iterations: 999] [last reward: -0.1415807455778122]\n",
      "[190/200] [loss_mean: 7.202404783248902] [rewards_mean: 152.64004489317807] [iterations: 999] [last reward: -0.005186272785067558]\n",
      "[191/200] [loss_mean: 6.908706684023212] [rewards_mean: 210.82757889060304] [iterations: 426] [last reward: 100.0]\n",
      "[192/200] [loss_mean: 7.096115721430524] [rewards_mean: 210.9630024196813] [iterations: 318] [last reward: 100.0]\n",
      "[193/200] [loss_mean: 6.777961975844332] [rewards_mean: 284.17889110886904] [iterations: 257] [last reward: 100.0]\n",
      "[194/200] [loss_mean: 7.0204543560743335] [rewards_mean: 247.52129685531295] [iterations: 399] [last reward: 100.0]\n",
      "[195/200] [loss_mean: 6.779496616012258] [rewards_mean: 263.2191762523319] [iterations: 580] [last reward: 100.0]\n",
      "[196/200] [loss_mean: 7.388427934108377] [rewards_mean: 181.66151510644704] [iterations: 566] [last reward: 100.0]\n",
      "[197/200] [loss_mean: 6.954866980853146] [rewards_mean: 294.40426809312896] [iterations: 291] [last reward: 100.0]\n",
      "[198/200] [loss_mean: 7.089357075214386] [rewards_mean: 83.95759356634517] [iterations: 999] [last reward: -0.035311009734869]\n",
      "[199/200] [loss_mean: 6.883449914209511] [rewards_mean: 217.44717158478625] [iterations: 445] [last reward: 100.0]\n"
     ]
    }
   ],
   "source": [
    "from itertools import count\n",
    "import numpy as np\n",
    "\n",
    "while len(memory) < BATCH_SIZE:\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    \n",
    "    while True:\n",
    "        action = torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device, dtype=torch.float32)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        if terminated:\n",
    "          next_state = None\n",
    "        else:\n",
    "          next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "        memory.push(state, action, next_state, reward)\n",
    "        \n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "env.close()\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\",render_mode='human')\n",
    "\n",
    "ep_losses = []\n",
    "\n",
    "with SummaryWriter(log_dir=f'duel_runs/{timestr}') as writer:\n",
    "    \n",
    "    while global_step < num_episodes:\n",
    "        state, info = env.reset(seed=global_step)\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * global_step / EPS_DECAY)\n",
    "\n",
    "        ep_reward = 0\n",
    "        ep_qvalues = 0\n",
    "        # ep_gif_frames = []\n",
    "\n",
    "        for t in count():\n",
    "\n",
    "            action = select_action(state, eps_threshold)\n",
    "            observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "            reward = torch.tensor([reward], device=device, dtype=torch.float32)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # ep_gif_frames.append(env.render())\n",
    "            ep_reward += reward.cpu().numpy().item()\n",
    "            ep_qvalues += policy_net(state).max(1)[0].item()\n",
    "\n",
    "            if terminated:\n",
    "              next_state = None\n",
    "            else:\n",
    "              next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "            memory.push(state, action, next_state, reward)\n",
    "            state = next_state\n",
    "\n",
    "            loss_scalar = optimize_model()\n",
    "            ep_losses.append(loss_scalar)\n",
    "\n",
    "            target_net_state_dict = target_net.state_dict()\n",
    "            policy_net_state_dict = policy_net.state_dict()\n",
    "\n",
    "            # Soft update of the target network's weights\n",
    "            # θ′ ← τ θ + (1 −τ )θ′\n",
    "            for key in policy_net_state_dict:\n",
    "                target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "            target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "            if done:\n",
    "                mean_rewards = ep_reward # / (t + 1)\n",
    "                print(f'[{global_step}/{num_episodes}]', f'[loss_mean: {np.mean(ep_losses)}]', f'[rewards_mean: {mean_rewards}]', f'[iterations: {t}]', f'[last reward: {reward.item()}]')\n",
    "                break\n",
    "            \n",
    "\n",
    "        loss_mean = np.mean(ep_losses)\n",
    "        \n",
    "        writer.add_hparams(\n",
    "\n",
    "            h_params,\n",
    "            {\n",
    "                'i_episode': global_step,\n",
    "                't': t + 1,\n",
    "                'Memory_len': len(memory),\n",
    "                'Threshold': eps_threshold,\n",
    "                'Loss': loss_mean, # loss_scalar,\n",
    "                'Qvalues': ep_qvalues / (t + 1),\n",
    "                'Reward': ep_reward / (t + 1),\n",
    "            }, name='.', global_step=global_step,\n",
    "        )\n",
    "        ep_losses = []\n",
    "        \n",
    "        global_step += 1 \n",
    "        writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da861513-02d6-4bcb-baff-e2f6af69fec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "save_model_dir = './duel_saved_models'\n",
    "\n",
    "if not os.path.exists(f'{save_model_dir}'):\n",
    "    os.mkdir(f'{save_model_dir}')\n",
    "if not os.path.exists(f'{save_model_dir}/{timestr}'):\n",
    "    os.mkdir(f'{save_model_dir}/{timestr}')\n",
    "\n",
    "torch.save(policy_net.state_dict(), f'{save_model_dir}/{timestr}/policy_net')\n",
    "torch.save(target_net.state_dict(), f'{save_model_dir}/{timestr}/target_net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8102a4-0bac-422f-b7d5-25c038cc2957",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "#from IPython import display\n",
    "\n",
    "#_, ax = plt.subplots(1, 1)\n",
    "\n",
    "#img = ax.imshow(env.render())\n",
    "env.close()\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\",render_mode='human')\n",
    "\n",
    "while True:\n",
    "  state, info = env.reset()\n",
    "  state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "  for t in count():\n",
    "    with torch.no_grad():\n",
    "      action = policy_net(state).max(1)[1].view(1,1)\n",
    "    \n",
    "    observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "    done = terminated or truncated\n",
    "\n",
    "    if terminated:\n",
    "      next_state = None\n",
    "    else:\n",
    "      next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    \n",
    "    state = next_state\n",
    "    \n",
    "    env.render()\n",
    "    #img.set_data(env.render()) \n",
    "    #ax.axis('off')\n",
    "    #display.display(plt.gcf())\n",
    "    #display.clear_output(wait=True)\n",
    "    if done:\n",
    "      break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
